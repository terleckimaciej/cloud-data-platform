# =========================================================
# pracuj-pl-pipeline  —  Project Requirements (Cloud Composer)
# =========================================================
# Plik zawiera pełną listę zależności wymaganych do uruchomienia
# pipeline’u danych (scraping → enrichment → Spark → BigQuery → ML).
# =========================================================

# --- Etap 1: Web scraping ---
requests-html==0.10.0
beautifulsoup4==4.12.3
lxml==5.3.0
lxml-html-clean==0.1.0
html5lib==1.1
pyppeteer==2.0.0
nest-asyncio==1.6.0          # asyncio support for requests-html
fake-useragent==1.5.1         # random user-agents to avoid blocking
tqdm==4.66.5                  # progress bars for long loops

# --- Etap 2: Zapis / odczyt z Google Cloud Storage ---
gcsfs==2024.6.0
google-cloud-storage==2.18.2

# --- Etap 3: Przetwarzanie i czyszczenie danych (Python / PySpark) ---
pandas==2.2.2
pyarrow==17.0.0
pyspark==3.5.1

# --- Etap 4: Integracja z BigQuery ---
google-cloud-bigquery==3.24.0
pandas-gbq==0.23.0

# --- Etap 5: ML / Analiza ( opcjonalnie BigQuery ML, Vertex AI ) ---
google-cloud-aiplatform==1.61.0

# --- Etap 6: Orkiestracja / Integracje (Airflow / Composer) ---
requests_html==0.10.0
beautifulsoup4==4.12.3
nest_asyncio==1.6.0
fake_useragent==1.5.1
gcsfs==2024.6.0

# --- Dodatkowe narzędzia ---
python-dateutil==2.9.0.post0  # datetime utilities
pytz==2024.1                  # timezone support
